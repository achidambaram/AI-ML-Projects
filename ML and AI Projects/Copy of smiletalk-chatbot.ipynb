{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":107,"status":"ok","timestamp":1648412328772,"user":{"displayName":"Arjun Chidambaran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG0IalJkYO557rcLIwrQWXpD2Ysv12_XT8mUhSSw=s64","userId":"15335744820933329132"},"user_tz":420},"id":"LuF_07RHEGQa","outputId":"5d41fe8d-56a5-4907-a01c-52432a0709f1"},"outputs":[{"output_type":"stream","name":"stdout","text":["Smile-Talk-Work ...\n"]}],"source":["print('Smile-Talk-Work ...')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AcnnEfmMKinw"},"outputs":[],"source":["import zipfile\n","import pandas as pd \n","import numpy as np\n","from IPython.display import display\n","from pprint import pprint\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","import string\n","import re\n","from sklearn.model_selection import train_test_split\n","from sklearn.utils import resample"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BpRlQ3MIHfh-"},"outputs":[],"source":["# Karim: how to access the file?\n","# with zipfile.ZipFile('/content/archive.zip', 'r') as zip_ref:\n","    #zip_ref.extractall('/content/Output')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":493},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1648396028998,"user":{"displayName":"Arjun Chidambaran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG0IalJkYO557rcLIwrQWXpD2Ysv12_XT8mUhSSw=s64","userId":"15335744820933329132"},"user_tz":420},"id":"1dfE0bMaHgUJ","outputId":"57df1ad9-153f-4deb-99b3-9c936680ab3d"},"outputs":[{"output_type":"display_data","data":{"text/plain":["         tweet_id   sentiment  \\\n","0      1956967341       empty   \n","1      1956967666     sadness   \n","2      1956967696     sadness   \n","3      1956967789  enthusiasm   \n","4      1956968416     neutral   \n","...           ...         ...   \n","39995  1753918954     neutral   \n","39996  1753919001        love   \n","39997  1753919005        love   \n","39998  1753919043   happiness   \n","39999  1753919049        love   \n","\n","                                                 content  \n","0      @tiffanylue i know  i was listenin to bad habi...  \n","1      Layin n bed with a headache  ughhhh...waitin o...  \n","2                    Funeral ceremony...gloomy friday...  \n","3                   wants to hang out with friends SOON!  \n","4      @dannycastillo We want to trade with someone w...  \n","...                                                  ...  \n","39995                                   @JohnLloydTaylor  \n","39996                     Happy Mothers Day  All my love  \n","39997  Happy Mother's Day to all the mommies out ther...  \n","39998  @niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...  \n","39999  @mopedronin bullet train from tokyo    the gf ...  \n","\n","[40000 rows x 3 columns]"],"text/html":["\n","  <div id=\"df-54ec83b3-30ba-4ddf-90de-905d887843cb\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>tweet_id</th>\n","      <th>sentiment</th>\n","      <th>content</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1956967341</td>\n","      <td>empty</td>\n","      <td>@tiffanylue i know  i was listenin to bad habi...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1956967666</td>\n","      <td>sadness</td>\n","      <td>Layin n bed with a headache  ughhhh...waitin o...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>1956967696</td>\n","      <td>sadness</td>\n","      <td>Funeral ceremony...gloomy friday...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>1956967789</td>\n","      <td>enthusiasm</td>\n","      <td>wants to hang out with friends SOON!</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>1956968416</td>\n","      <td>neutral</td>\n","      <td>@dannycastillo We want to trade with someone w...</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>39995</th>\n","      <td>1753918954</td>\n","      <td>neutral</td>\n","      <td>@JohnLloydTaylor</td>\n","    </tr>\n","    <tr>\n","      <th>39996</th>\n","      <td>1753919001</td>\n","      <td>love</td>\n","      <td>Happy Mothers Day  All my love</td>\n","    </tr>\n","    <tr>\n","      <th>39997</th>\n","      <td>1753919005</td>\n","      <td>love</td>\n","      <td>Happy Mother's Day to all the mommies out ther...</td>\n","    </tr>\n","    <tr>\n","      <th>39998</th>\n","      <td>1753919043</td>\n","      <td>happiness</td>\n","      <td>@niariley WASSUP BEAUTIFUL!!! FOLLOW ME!!  PEE...</td>\n","    </tr>\n","    <tr>\n","      <th>39999</th>\n","      <td>1753919049</td>\n","      <td>love</td>\n","      <td>@mopedronin bullet train from tokyo    the gf ...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>40000 rows Ã— 3 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-54ec83b3-30ba-4ddf-90de-905d887843cb')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-54ec83b3-30ba-4ddf-90de-905d887843cb button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-54ec83b3-30ba-4ddf-90de-905d887843cb');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{}},{"output_type":"execute_result","data":{"text/plain":["tweet_id      int64\n","sentiment    object\n","content      object\n","dtype: object"]},"metadata":{},"execution_count":6}],"source":["# Karim: are you linking with GDrive? I think better if someone hosted in Gdrive then grant public access\n","dataset = pd.read_csv('/content/Output/tweet_emotions.csv')\n","display(dataset)\n","dataset.dtypes\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yFRy3kr-uKG7"},"outputs":[],"source":["# dataset.sentiment.value_counts()\n","# target_class = 8\n","\n","# classes_ids = {name:ids for name, ids in zip(set(dataset.sentiment.to_list()),range(len(set(dataset.sentiment.to_list()))))}\n","# inv_classes_ids = {value:key for key, value in zip(list(classes_ids.keys()), list(classes_ids.values()))}\n","\n","# pprint(classes_ids)\n","\n","# target_majority = dataset[dataset.sentiment==inv_classes_ids[target_class]]\n","\n","# for cl in range(len(classes_ids)):\n","#     train_minority = dataset[dataset.sentiment==inv_classes_ids[cl]]\n","#     train_minority_upsampled = resample(train_minority, replace=True, n_samples=len(target_majority), random_state=123)\n","#     if cl == 0:\n","#         dataset_upsampled = pd.concat([train_minority_upsampled, target_majority])\n","#         #train_upsampled = pd.concat([train_upsampled, ])\n","#     if cl>0 and cl!=target_class:\n","#         dataset_upsampled = pd.concat([train_minority_upsampled, dataset_upsampled])\n","\n","# print(dataset_upsampled['sentiment'].value_counts()) \n","# dataset_upsampled = dataset_upsampled.sample(frac=1).reset_index(drop=True)\n","# display(dataset_upsampled.head(5))"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MHOj--3WHg61","outputId":"4b8bbbf0-2a5a-4a5b-f51b-d115f98b4bd7","executionInfo":{"status":"ok","timestamp":1648916911046,"user_tz":420,"elapsed":193405,"user":{"displayName":"Arjun Chidambaran","userId":"15335744820933329132"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["5\n","['empty' 'sadness' 'sadness' ... 'love' 'happiness' 'love']\n","['empty' 'sadness' 'enthusiasm' 'neutral' 'worry' 'surprise' 'love' 'fun'\n"," 'hate' 'happiness' 'boredom' 'relief' 'anger']\n","13\n","0\n","[[2]\n"," [3]\n"," [1]\n"," ...\n"," [2]\n"," [2]\n"," [2]]\n","[[3]\n"," [3]\n"," [1]\n"," ...\n"," [5]\n"," [2]\n"," [2]]\n","1000\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, None, 64)          64000     \n","                                                                 \n"," bidirectional (Bidirectiona  (None, 128)              66048     \n"," l)                                                              \n","                                                                 \n"," dropout (Dropout)           (None, 128)               0         \n","                                                                 \n"," dense (Dense)               (None, 64)                8256      \n","                                                                 \n"," dropout_1 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_1 (Dense)             (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 139,149\n","Trainable params: 139,149\n","Non-trainable params: 0\n","_________________________________________________________________\n","219/219 [==============================] - 63s 261ms/step - loss: 2.3016 - accuracy: 0.2169 - val_loss: 2.1575 - val_accuracy: 0.2628\n","WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_1\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_1 (Embedding)     (None, None, 64)          64000     \n","                                                                 \n"," lstm_1 (LSTM)               (None, 64)                33024     \n","                                                                 \n"," dropout_2 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_2 (Dense)             (None, 64)                4160      \n","                                                                 \n"," dropout_3 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_3 (Dense)             (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 102,029\n","Trainable params: 102,029\n","Non-trainable params: 0\n","_________________________________________________________________\n","219/219 [==============================] - 33s 144ms/step - loss: 2.2551 - accuracy: 0.2149 - val_loss: 2.1342 - val_accuracy: 0.2689\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer lstm_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","Model: \"sequential_2\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding_2 (Embedding)     (None, None, 64)          64000     \n","                                                                 \n"," bidirectional_1 (Bidirectio  (None, 128)              66048     \n"," nal)                                                            \n","                                                                 \n"," dropout_4 (Dropout)         (None, 128)               0         \n","                                                                 \n"," dense_4 (Dense)             (None, 64)                8256      \n","                                                                 \n"," dropout_5 (Dropout)         (None, 64)                0         \n","                                                                 \n"," dense_5 (Dense)             (None, 13)                845       \n","                                                                 \n","=================================================================\n","Total params: 139,149\n","Trainable params: 139,149\n","Non-trainable params: 0\n","_________________________________________________________________\n","219/219 [==============================] - 63s 270ms/step - loss: 2.3016 - accuracy: 0.2188 - val_loss: 2.1434 - val_accuracy: 0.2699\n"]}],"source":["# import tensorflow as tf\n","# import re\n","\n","# emotions = df_tweet['sentiment'].tolist()\n","# contents = df_tweet['content'].tolist()\n","# unique_emotions = df_tweet.sentiment.unique()\n","# emotions_in_number = {}\n","# emotions_dataset = []\n","\n","# for i,e in enumerate(unique_emotions):\n","#   emotions_in_number.update({e: i})\n","\n","# for e in emotions:\n","#   emotions_dataset.append(emotions_in_number[e])\n","\n","\n","# def clean_text(text):\n","#   text = re.sub(r\"\\'m\", \" am\", text)\n","#   text = re.sub(r\"\\'s\", \" is\", text)\n","#   text = re.sub(r\"\\'re\", \" are\", text)\n","#   text = re.sub(r\"\\'ll\", \" will\", text)\n","#   text = re.sub(r\"\\'d\", \" would\", text)\n","#   text = re.sub(r\"won't\", \"will not\", text)\n","#   text = re.sub(r\"can't\", \"cannot\", text)\n","#   text = re.sub(r\"\\.\", \" . \", text)\n","#   text = re.sub(r\"\\?\", \" ? \", text)\n","#   text = re.sub(r\"!\", \" ! \", text)\n","#   text = re.sub(r\"/\", \" / \", text)\n","#   text = re.sub(r\",\", \" , \", text)\n","#   text = re.sub(r'\"', ' \" ', text)\n","#   text = re.sub(r\"-\", \" - \", text)\n","\n","#   text = re.sub(r\"[-{}+=|?'()\\:@]\", \"\", text)\n","#   return text\n","\n","# content_dataset = []\n","\n","# for line in contents:\n","#   content_dataset.append(clean_text(line))\n","\n","\n","# inputs_dataset = tf.data.Dataset.from_tensor_slices(content_dataset)\n","\n","# tokenizer = tf.keras.layers.TextVectorization(\n","#     max_tokens=5000,\n","#     output_sequence_length=50\n","# )\n","\n","# tokenizer.adapt(inputs_dataset)\n","\n","# def prepare_dataset(input, output):\n","#   input = tokenizer(input)\n","#   pad_len = 200 - len(input)\n","#   paddings = [0] * pad_len\n","#   paddings = tf.cast(paddings, dtype=tf.int64)\n","#   input = input + paddings\n","#   return input, output\n","\n","# dataset = tf.data.Dataset.from_tensor_slices((content_dataset, emotions_dataset))\n","# dataset = dataset.map(prepare_dataset, num_parallel_calls=tf.data.AUTOTUNE)\n","# dataset = dataset.shuffle(1000)\n","# dataset = dataset.padded_batch(64, drop_remainder=True)\n","# dataset = dataset.prefetch(tf.data.AUTOTUNE)\n","\n","# for input, output in dataset.take(2):\n","#   print(input.numpy())\n","\n","dataset = pd.read_csv('/content/Output/tweet_emotions.csv')\n","# print(dataset_upsampled.dtypes)\n","# dataset_upsampled.head()\n","\n","print(5)\n","\n","content = dataset['content']\n","\n","contentArr = content.to_numpy()\n","\n","\n","contentArr_length = len(contentArr)\n","\n","sentiment = dataset['sentiment']\n","sentimentArr = sentiment.to_numpy()\n","\n","print(sentimentArr)\n","# sentimentArr = sentimentArr[0:contentArr_length]\n","\n","sentimentArr_length = len(sentimentArr)\n","\n","sentimentUniqueArr = sentiment.unique()\n","print(sentimentUniqueArr)\n","\n","sentimentArr_unique_length = len(sentimentUniqueArr)\n","\n","print(sentimentArr_unique_length)\n","\n","tokenizer = Tokenizer(num_words=1000, oov_token='<oov>')\n","sentimentTokenizer = Tokenizer(num_words=13)\n","\n","\n","def vocab_count(arr):\n","  vocab_size = 0\n","\n","  for sentences in arr:\n","    for j in sentences:\n","      vocab_size += 1\n","\n","  return vocab_size\n","\n","  print(vocab_count(contentArr))\n","\n","\n","def prepare_sentence(sentence):\n","  return re.sub(r'([{}])'.format(string.punctuation),r' ', sentence)\n","\n","def tokenization(arr):\n","  prepared_arr = []\n","  for sentence in arr:\n","    newSentence = prepare_sentence(sentence)\n","    prepared_arr.append(newSentence)\n","  \n","  tokenizer.fit_on_texts(prepared_arr)\n","  sequences = tokenizer.texts_to_sequences(prepared_arr)\n","  return pad_sequences(sequences)\n","\n","def sentimentTokenization(arr):\n","  prepared_arr = []\n","  for sentence in arr:\n","    newSentence = prepare_sentence(sentence)\n","    prepared_arr.append(newSentence)\n","  \n","  sentimentTokenizer.fit_on_texts(prepared_arr)\n","  sequences = sentimentTokenizer.texts_to_sequences(prepared_arr)\n","  return pad_sequences(sequences)\n","\n","content_length = int(0.7 * len(contentArr))\n","sentiment_length = int(0.7 * len(sentimentArr))\n","\n","\n","contentNewArr = tokenization(contentArr)\n","sentimentInNumber = {}\n","sentimentNewArr = sentimentTokenization(sentimentArr)\n","unwanted_idx = []\n","\n","for i, sentiment in enumerate(sentimentNewArr):\n","  if(sentiment > 13):\n","    unwanted_idx.append(sentiment)\n","print(len(unwanted_idx))\n","\n","# for i,e in enumerate(sentimentUniqueArr):\n","#   sentimentInNumber.update({e: i})\n","\n","# for i in sentimentArr:\n","#   sentimentNewArr.append(sentimentInNumber[i])\n","\n","# print(contentArr)\n","\n","training_sentences, testing_sentences, training_labels, testing_labels = train_test_split(contentNewArr, sentimentNewArr, test_size=0.3)\n","\n","print(training_labels)\n","print(testing_labels)\n","\n","# training_labels = training_labels.reshape(-1, 13)\n","# testing_labels = testing_labels.reshape(-1, 13)\n","\n","# print(training_sentences.shape, training_labels.shape)\n","# print(testing_sentences.shape, testing_labels.shape)\n","\n","\n","# original_training_sentences = contentArr[0:content_length]\n","# original_testing_sentences = contentArr[content_length:]\n","\n","# training_labels = sentimentArr[0:sentiment_length]\n","# testing_labels = sentimentArr[sentiment_length:]\n","\n","# len(original_testing_sentences)\n","# len(training_labels)\n","\n","\n","\n","\n","# trains_sentences = np.array(training_sentences)\n","# testings_sentences = np.array(testing_sentences)\n","\n","# trains_sentences = training_sentences.reshape(-1, 1)\n","# testings_sentences = testing_sentences.reshape(-1, 1)\n","\n","# train = {\"training\":trains_sentences, \"labels\":training_labels}\n","# test = {\"testing\":testings_sentences, \"labels\":testing_labels}\n","# training_dataset = pd.DataFrame(data=train)\n","# val_dataset = pd.DataFrame(data=test)\n","\n","# training_dataset = pd.DataFrame(data=d)\n","#remove if necessary\n","\n","# trainl_labels = np.array(training_labels)\n","# testingl_labels = np.array(testing_labels)\n","\n","\n","\n","# Check after resolving Tuple Issue\n","\n","# print(trains_sentences.shape())\n","# print(testings_sentences.shape())\n","\n","# print(len(trains_sentences))\n","# print(len(trainl_labels))\n","\n","# print(len(testings_sentences))\n","# print(len(testingl_labels))\n","\n","\n","\n","# training_dataset = tf.data.Dataset.from_tensor_slices((training_sentences, training_labels))\n","# training_dataset = training_dataset.shuffle(1000)\n","# training_dataset = training_dataset.padded_batch(64, drop_remainder=True)\n","# training_dataset = training_dataset.prefetch(tf.data.AUTOTUNE)\n","\n","# val_dataset = tf.data.Dataset.from_tensor_slices((testing_sentences, testing_labels))\n","# val_dataset = val_dataset.shuffle(1000)\n","# val_dataset = val_dataset.padded_batch(64, drop_remainder=True)\n","# val_dataset = val_dataset.prefetch(tf.data.AUTOTUNE)\n","\n","# training_array = np.array(training_dataset)\n","# val_array = np.array(val_dataset)\n","\n","contentArr = content.to_numpy()\n","# print(contentArr)\n","\n","vocab_size = tokenizer.num_words\n","print(vocab_size)\n","\n","# model = tf.keras.Sequential()\n","# model.add(tf.keras.layers.Embedding(input_dim=1000, output_dim=100))\n","# model.add(tf.keras.layers.SpatialDropout1D(0.2))\n","# model.add(tf.keras.layers.LSTM(100, activation='relu'))\n","# # model.add(tf.keras.layers.Dense(100))\n","# model.add(tf.keras.layers.Dense(13, activation='softmax'))\n","\n","embedding_dim = 64\n","\n","model1 = tf.keras.Sequential()\n","model1.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n","# model.add(tf.keras.layers.Dropout(0.5))\n","model1.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))))\n","# commented recently ---> model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5))))\n","# model.add(tf.keras.layers.Dropout(0.3))\n","# commented recently ---> model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5))))\n","# model.add(tf.keras.layers.GRU(64, return_sequences=True, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n","# tf.keras.layers.Dropout(0.3)\n","# # model.add(tf.keras.layers.GRU(64, recurrent_dropout = 0.3, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n","model1.add(tf.keras.layers.Dropout(0.3))\n","model1.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n","model1.add(tf.keras.layers.Dropout(0.3))\n","model1.add(tf.keras.layers.Dense(13, activation='softmax'))\n","\n","model1.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model1.summary()\n","\n","history1 = model1.fit(training_sentences, training_labels, batch_size=128, validation_data=(testing_sentences, testing_labels), epochs=1)\n","\n","model1.save('m1.h5')  \n","model1.load_weights('m1.h5')\n","\n","# model.load_weights('chatbot.h5')\n","\n","\n","\n","model2 = tf.keras.Sequential()\n","model2.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n","model2.add(tf.keras.layers.LSTM(64, activation='relu'))\n","model2.add(tf.keras.layers.Dropout(0.3))\n","model2.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n","model2.add(tf.keras.layers.Dropout(0.3))\n","model2.add(tf.keras.layers.Dense(13, activation='softmax'))\n","model2.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model2.summary()\n","\n","history2 = model2.fit(training_sentences, training_labels, batch_size=128, validation_data=(testing_sentences, testing_labels), epochs=1)\n","\n","model2.save('m2.h5')  \n","model2.load_weights('m2.h5')\n","\n","\n","model3 = tf.keras.Sequential()\n","model3.add(tf.keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim))\n","# model.add(tf.keras.layers.Dropout(0.5))\n","model3.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001))))\n","# commented recently ---> model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5))))\n","# model.add(tf.keras.layers.Dropout(0.3))\n","# commented recently ---> model.add(tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5))))\n","# model.add(tf.keras.layers.GRU(64, return_sequences=True, recurrent_dropout = 0.4, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n","# tf.keras.layers.Dropout(0.3)\n","# # model.add(tf.keras.layers.GRU(64, recurrent_dropout = 0.3, kernel_regularizer=tf.keras.regularizers.l1_l2(l1=1e-5, l2=1e-4), activity_regularizer=tf.keras.regularizers.l2(1e-5)))\n","model3.add(tf.keras.layers.Dropout(0.3))\n","model3.add(tf.keras.layers.Dense(64, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)))\n","model3.add(tf.keras.layers.Dropout(0.3))\n","model3.add(tf.keras.layers.Dense(13, activation='softmax'))\n","\n","model3.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model3.summary()\n","\n","history3 = model3.fit(training_sentences, training_labels, batch_size=128, validation_data=(testing_sentences, testing_labels), epochs=1)\n","\n","model3.save('m3.h5')  \n","model3.load_weights('m3.h5')\n","\n","# model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3), loss=tf.keras.losses.SparseCategoricalCrossentropy())\n","# model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","\n","\n","# training_dataset = pd.DataFrame(data=(trains_sentences, training_labels))\n","# val_dataset = pd.DataFrame(data=(testings_sentences, testing_labels))\n","\n","# training_array = training_dataset.to_numpy()\n","# val_array = val_dataset.to_numpy()\n","\n","# final_train_sentences = np.array(training_sentences)\n","# final_testing_sentences = np.array(testing_sentences)\n","# final_testing_labels = np.array(testing_labels)\n","\n","# # print(len(final_train_sentences))\n","# # print(len(final_train_labels))\n","\n","# print(len(final_testing_sentences))\n","# print(len(final_testing_labels))\n","\n","\n","# final_train_sentences = final_train_sentences.reshape(-1, 1)\n","# final_testing_sentences = final_testing_sentences.reshape(-1, 1)\n","\n","# print(training_sentences)\n","# print(training_labels)\n","# print(testing_sentences)\n","# print(testing_labels)\n","\n","# model.fit(trains_sentences, training_labels, validation_data=(testings_sentences, testing_labels), epochs=37)\n","\n","\n","# test = \"Wondering why I'm awake at 7am,writing a new ,plotting my evil secret plots muahahaha...oh damn it,not secret anymore\"\n","# test_array = np.array(test)\n","# print(test_array)"]},{"cell_type":"code","source":["from sklearn.metrics import accuracy_score\n","import random\n","from tensorflow import keras\n","\n","model1 = keras.models.load_model('m1.h5')\n","model2 = keras.models.load_model('m2.h5')\n","model3 = keras.models.load_model('m3.h5')\n","\n","models = [model1, model2, model3]\n","\n","# ensemble_prediction = np.argmax(sumPredictions, axis=1)\n","\n","def ensembleArr(x):\n","  # prediction1 = model1.predict(x)\n","  # prediction2 = model2.predict(x)\n","  # prediction3 = model3.predict(x)\n","\n","  # # onePred = np.reshape(prediction1, (-1, 1))\n","  # # secondPred = np.reshape(prediction2, (-1, 1))\n","  # # thirdPred = np.reshape(prediction3, (-1, 1))\n","\n","  # combinedPreds = np.concatenate((prediction1, prediction2), axis=0)\n","  # totalPrediction = np.concatenate((combinedPreds, prediction3), axis=0)\n","  # totalPrediction = np.array(totalPrediction)\n","  # return totalPrediction\n","\n","  predictions = [model.predict(x) for model in models]\n","  predictions = np.array(predictions)\n","\n","  sumPredictions = np.sum(predictions, axis=0)\n","\n","  ensemble_prediction = np.argmax(sumPredictions, axis=1)\n","\n","  return ensemble_prediction \n","\n","ensemble_arr = ensembleArr(training_sentences)\n","print(ensemble_arr)\n","\n","datafin = pd.DataFrame(ensemble_arr)\n","print(datafin.head())\n","\n","print(training_labels)\n","\n","model = tf.keras.Sequential()\n","model.add(tf.keras.layers.Embedding(input_dim=len(training_sentences), output_dim=64))\n","model.add(tf.keras.layers.Dense(64, activation='relu'))\n","model.add(tf.keras.layers.Dense(1, activation='softmax'))\n","\n","model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","model.summary()\n","\n","history = model.fit(ensembleArr(testing_sentences), testing_labels, epochs=10)\n","# mse = tf.keras.losses.MeanSquaredError()\n","\n","# accuracy1 = mse(testing_labels, prediction1).numpy()\n","# accuracy2 = mse(testing_labels, prediction2).numpy()\n","# accuracy3 = mse(testing_labels, prediction3).numpy()\n","# ensemble_accuracy = mse(testing_labels, ensemble_prediction).numpy()\n","\n","# accuracy1 = sum(prediction1 == testing_labels)/len(prediction1)\n","# accuracy2 = sum(prediction2 == testing_labels)/len(prediction2)\n","# accuracy3 = sum(prediction3 == testing_labels)/len(prediction3)\n","# ensemble_accuracy = sum(ensemble_prediction == testing_labels)/len(ensemble_prediction)\n","\n","# print(\"Model1 Accuracy Loss {}\".format(accuracy1))\n","# print(\"Model2 Accuracy Loss {}\".format(accuracy2))\n","# print(\"Model3 Accuracy Loss {}\".format(accuracy3))\n","# print(\"Ensemble Model Accuracy Loss {}\".format(ensemble_accuracy))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"6nwelMjTq4Jm","executionInfo":{"status":"error","timestamp":1648923512672,"user_tz":420,"elapsed":3658,"user":{"displayName":"Arjun Chidambaran","userId":"15335744820933329132"}},"outputId":"eeacbb77-531c-44f3-d294-024a89ec796e"},"execution_count":null,"outputs":[{"output_type":"error","ename":"OSError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-6ccac529b518>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmodel1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm1.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mmodel2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm2.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mmodel3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'm3.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    202\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mIOError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'No file or directory found at {filepath_str}'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misdir\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mOSError\u001b[0m: No file or directory found at m1.h5"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":367},"executionInfo":{"elapsed":754,"status":"ok","timestamp":1648475758995,"user":{"displayName":"Arjun Chidambaran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG0IalJkYO557rcLIwrQWXpD2Ysv12_XT8mUhSSw=s64","userId":"15335744820933329132"},"user_tz":420},"id":"U0RuitlEDYdM","outputId":"5e376425-0971-4e95-ecc5-3541095893c7"},"outputs":[{"output_type":"stream","name":"stdout","text":["dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/matplotlib/text.py:1165: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n","  if s != self._text:\n"]},{"output_type":"display_data","data":{"text/plain":["<Figure size 432x288 with 1 Axes>"],"image/png":"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\n"},"metadata":{"needs_background":"light"}}],"source":["print(history1.history.keys())\n","import matplotlib.pyplot as plt\n","\n","plt.plot(history1.history['loss'])\n","plt.plot(history1.history['val_loss'])\n","plt.title('model loss')\n","plt.ylabel('loss')\n","plt.xlabel('epoch')\n","plt.legend(['train', 'test'], loc='upper left')\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":2083,"status":"ok","timestamp":1647776452264,"user":{"displayName":"Fyaz Rayat","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GgBj6Dqh2PgC5keTfQ2HnFM_0baQxIIYU6ngY_8Hg=s64","userId":"12768280763056731303"},"user_tz":-360},"id":"ntD_SveUC0HP","outputId":"5fbedd74-3328-4150-f9e0-c9c5ec77f7c4"},"outputs":[{"name":"stdout","output_type":"stream","text":["WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","WARNING:tensorflow:Layer gru_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n","[[4.4284061e-02 2.2680397e-01 9.6204552e-05 2.9264960e-02 9.0456724e-02\n","  2.1569803e-01 1.2188174e-01 3.4843415e-03 2.1615556e-06 4.0737040e-02\n","  2.0407822e-02 5.4572184e-02 1.5231074e-01]]\n","1\n","{'num_words': 13, 'filters': '!\"#$%&()*+,-./:;<=>?@[\\\\]^_`{|}~\\t\\n', 'lower': True, 'split': ' ', 'char_level': False, 'oov_token': None, 'document_count': 67717, 'word_counts': '{\"sadness\": 5209, \"anger\": 5209, \"love\": 5209, \"relief\": 5209, \"worry\": 5209, \"neutral\": 5209, \"enthusiasm\": 5209, \"boredom\": 5209, \"empty\": 5209, \"fun\": 5209, \"hate\": 5209, \"surprise\": 5209, \"happiness\": 5209}', 'word_docs': '{\"sadness\": 5209, \"anger\": 5209, \"love\": 5209, \"relief\": 5209, \"worry\": 5209, \"neutral\": 5209, \"enthusiasm\": 5209, \"boredom\": 5209, \"empty\": 5209, \"fun\": 5209, \"hate\": 5209, \"surprise\": 5209, \"happiness\": 5209}', 'index_docs': '{\"1\": 5209, \"2\": 5209, \"3\": 5209, \"4\": 5209, \"5\": 5209, \"6\": 5209, \"7\": 5209, \"8\": 5209, \"9\": 5209, \"10\": 5209, \"11\": 5209, \"12\": 5209, \"13\": 5209}', 'index_word': '{\"1\": \"sadness\", \"2\": \"anger\", \"3\": \"love\", \"4\": \"relief\", \"5\": \"worry\", \"6\": \"neutral\", \"7\": \"enthusiasm\", \"8\": \"boredom\", \"9\": \"empty\", \"10\": \"fun\", \"11\": \"hate\", \"12\": \"surprise\", \"13\": \"happiness\"}', 'word_index': '{\"sadness\": 1, \"anger\": 2, \"love\": 3, \"relief\": 4, \"worry\": 5, \"neutral\": 6, \"enthusiasm\": 7, \"boredom\": 8, \"empty\": 9, \"fun\": 10, \"hate\": 11, \"surprise\": 12, \"happiness\": 13}'}\n"]}],"source":["from tensorflow.keras.models import load_model\n","import numpy as np\n","\n","chatbot = load_model('chatbot.h5')\n","\n","test_sentence = ['i do not feel well']\n","\n","sequence = tokenizer.texts_to_sequences(test_sentence)\n","\n","predictions = model.predict(sequence)\n","print(predictions)\n","print(np.argmax(predictions))\n","s = np.argmax(predictions)\n","idx2sentiment = {}\n","\n","for i,s in enumerate(sentimentArr):\n","  idx2sentiment.update({i:s})\n","\n","print(sentimentTokenizer.get_config())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3169,"status":"ok","timestamp":1647440507657,"user":{"displayName":"Arjun Chidambaran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG0IalJkYO557rcLIwrQWXpD2Ysv12_XT8mUhSSw=s64","userId":"15335744820933329132"},"user_tz":420},"id":"9-DmzrIC1naL","outputId":"6bbd0832-4669-466d-f023-8df95721bb37"},"outputs":[{"name":"stdout","output_type":"stream","text":["{'i': 1, 'love': 2, 'the': 3, 'cat': 4, 'a': 5, 'lot': 6, 'dog': 7, 'to': 8, 'my': 9, 'heart': 10}\n","[[1, 2, 5, 6], [3, 7, 9, 10], [1, 8, 9, 10, 2]]\n","[[ 0  1  2  5  6]\n"," [ 0  3  7  9 10]\n"," [ 1  8  9 10  2]]\n"]}],"source":["import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","sentences = ['I love the cat a lot', 'I love the dog to my heart']\n","\n","tokenizer = Tokenizer(num_words=100)\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(word_index)\n","\n","test = ['I love you a lot', 'The dog is my heart', 'I do not hold cats to my heart, love']\n","sequence = tokenizer.texts_to_sequences(test)\n","print(sequence)\n","\n","padded_sequence = pad_sequences(sequence)\n","print(padded_sequence)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16,"status":"ok","timestamp":1646612363724,"user":{"displayName":"Arjun Chidambaran","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GiG0IalJkYO557rcLIwrQWXpD2Ysv12_XT8mUhSSw=s64","userId":"15335744820933329132"},"user_tz":480},"id":"ohZ7lhxbb-0z","outputId":"4bf5cefc-c247-4cf5-ffd2-c06b53e45607"},"outputs":[{"name":"stdout","output_type":"stream","text":["(TensorSpec(shape=(64, None), dtype=tf.int64, name=None), TensorSpec(shape=(64,), dtype=tf.int32, name=None))\n"]}],"source":["print(dataset.element_spec)"]},{"cell_type":"markdown","metadata":{"id":"LYXwhpv8HSFs"},"source":["# Creating the Model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JSthVWO4dIsq"},"outputs":[],"source":["import tensorflow as tf\n","import matplotlib.pyplot as plt\n","import tensorflow_datasets as tfd\n","from keras.layers.recurrent import LSTM"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"name":"Copy of smiletalk-chatbot.ipynb","provenance":[{"file_id":"1FxiiG7X4Teh5akYgdhbXYW1GMTsG1Fdd","timestamp":1647873277896}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}